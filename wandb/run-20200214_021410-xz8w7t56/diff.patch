diff --git a/Net.py b/Net.py
index 52f64d984..2fb3064bd 100644
--- a/Net.py
+++ b/Net.py
@@ -9,15 +9,16 @@ class double(nn.Module):
         super(double, self).__init__()
         self.net1 = Net()
         self.net2 = Net()
-        self.fc1 = nn.Linear(8*8*32, 400)
-        self.fc2 = nn.Linear(400*2, 100)
+        self.fc1 = nn.Linear(7*7*32, 400)
+        self.fc2 = nn.Linear(400, 100)
         self.fc3 = nn.Linear(100, 19)
 
     def forward(self, x, y):
         x = self.fc1(self.net1(x))
         y = self.fc1(self.net1(y))
 
-        z = torch.cat([x,y], dim = 1)
+        # z = torch.cat([x,y], dim = 1)
+        z = x+y
         z = self.fc2(z)
         z = self.fc3(z)
         return z
@@ -34,7 +35,7 @@ class Net(nn.Module):
       x = self.conv1(x)
       x = self.pool(self.conv2(x))
       x = self.pool(self.conv3(x))
-      x = x.view(-1,8*8*32)
+      x = x.view(-1,7*7*32)
       return x
 
 class BnReluConv(nn.Module):
diff --git a/__pycache__/Net.cpython-36.pyc b/__pycache__/Net.cpython-36.pyc
index fdf35b1e9..d4f1d2c40 100644
Binary files a/__pycache__/Net.cpython-36.pyc and b/__pycache__/Net.cpython-36.pyc differ
diff --git a/data/annotation/train_pair.txt b/data/annotation/train_pair.txt
index 63146d8c2..83dd4665a 100644
Binary files a/data/annotation/train_pair.txt and b/data/annotation/train_pair.txt differ
diff --git a/make_pair.py b/make_pair.py
index 6bfb150fa..6e32c5cf0 100644
--- a/make_pair.py
+++ b/make_pair.py
@@ -18,8 +18,8 @@ def main():
     
     pair = []
     for _ in range(1000):
-        for i in range(10):
-            for j in range(10):
+        for i in range(0,10):
+            for j in range(0,10):
                 
                 name1 = sep_label[i][random.randrange(0,min_n[i])]
                 name2 = sep_label[j][random.randrange(0,min_n[j])]
diff --git a/train.py b/train.py
index 26ce7afdc..b0db5a8ce 100644
--- a/train.py
+++ b/train.py
@@ -16,7 +16,7 @@ import torch.optim as optim
 
 from Net import double 
 from Mydataset import MyDataSet, ValDataSet
-WANDB = True
+WANDB = 1
 if WANDB == True:
     import wandb
     wandb.init(project = "mnist_add")
@@ -30,14 +30,15 @@ def main():
 
     train_transform = transforms.Compose(
         [    # 360度ランダムで画像を回転する
-            transforms.Resize((32,32)),
+            # transforms.Resize((32,32)),
             transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0),
             transforms.ToTensor(),
             transforms.Normalize((0.5, ), (0.5,))])
 
     test_transform = transforms.Compose(
-            [transforms.Resize((32,32)),
-           transforms.ToTensor(),
+             [
+            # transforms.Resize((32,32)),
+            transforms.ToTensor(),
             transforms.Normalize((0.5,), (0.5,))])
     train_set = MyDataSet(train_transform)
     train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True,drop_last=True)
@@ -53,7 +54,7 @@ def main():
     #criterion = nn.SmoothL1Loss()
     criterion = nn.CrossEntropyLoss()
 
-    optimizer = optim.Adam(net.parameters(), lr = 0.01)
+    optimizer = optim.Adam(net.parameters(), lr = 0.001)
     # optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum=0.9)
     #optimizer = optim.SGD(net.parameters(), lr = 0.008, momentum=0.9, weight_decay = 0.008)
 
@@ -136,11 +137,11 @@ def main():
             for j in range(10):
                 print(i,j,"{}/{},{:.1f}%".format(result[i][j],result_num[i][j],result[i][j]/result_num[i][j]*100))
 
-        if WANDB = True:
+        if WANDB == True:
             wandb_log = {}
-            wandb["epoch"] = epoch
-            wandb["train_acc"] = correct_train/total_train*100
-            wandb["test_acc"] = correct_test/total_test*100
+            wandb_log["epoch"] = epoch
+            wandb_log["train_acc"] = correct_train/total_train*100
+            wandb_log["test_acc"] = correct_test/total_test*100
             wandb.log(wandb_log)
 
     net = net.cpu()
diff --git a/weight/classificate.pth b/weight/classificate.pth
index 45dfd73ad..101b2b55b 100644
Binary files a/weight/classificate.pth and b/weight/classificate.pth differ
